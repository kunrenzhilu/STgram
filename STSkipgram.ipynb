{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "seed = 1\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "import os \n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "import h5py\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy.ma as ma\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import load_data, DataLoader, DataLoader_time\n",
    "from parser import get_parser\n",
    "from utils import norm, normalize, is_normalized_matrix, extract_data, save_args, load_args, \\\n",
    "    save_embeddings, load_embeddings, DataStruct, save_model_tf, save_best_tf, load_model_tf\n",
    "from train import get_train_data\n",
    "from logger import Logger\n",
    "from model import init_params, crossentropy, choose_emb, choose_geo_loss, STSkipgram\n",
    "from multiprocess_tools import multiprocess_compute_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_parser(['--CITY', 'NYC', '--LOG_DIR', 'log_test', '--normalize_weight', '--WITH_TIME', '--WITH_GPS', '--WITH_TIMESTAMP', \n",
    "                   '--geo_reg_type', 'l2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from /home/haibin2/data/checkins/data/NYC_INTV_processed_voc5_len2_setting_WITH_GPS_WITH_TIME_WITH_USERID.pk\n",
      "args.pattern: hand\n",
      "indices setting : WITH_TIME normalize_weight WITH_GPS WITH_TIMESTAMP resume\n",
      "indices: [0, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "origin_data, dicts = load_data(os.path.join(args.ROOT, 'data','{}_INTV_processed_voc5_len2_setting_WITH_GPS_WITH_TIME_WITH_USERID.pk'.format(args.CITY) ))\n",
    "args.vocabulary_size = dicts.vocabulary_size\n",
    "data, idx = extract_data(origin_data, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode:both, size:(162302, 2, 5) size:(162302, 2, 5) total size:(324604, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "train_data = get_train_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_func(mat, sizes):\n",
    "    from functools import reduce\n",
    "    indices = [0]+[reduce(lambda x,y:x+y, sizes[:i]) for i in range(1,len(sizes)+1)]\n",
    "    tmp = list()\n",
    "    for i in range(len(indices)-1):\n",
    "        start, end = indices[i], indices[i+1]\n",
    "        tmp.append(mat[:,start:end])\n",
    "    return tmp\n",
    "\n",
    "class STSkipgram(object):\n",
    "    def __init__(self, args, pretrained_emb=None, pretrained_weight=None, pretrained_time=None):\n",
    "        self.pretrained_emb   = init_params((args.vocabulary_size, args.sem_dim+args.geo_dim+args.free_dim), pretrained_emb)\n",
    "        self.pretrained_weight= init_params((args.vocabulary_size, args.sem_dim+args.geo_dim+args.free_dim), pretrained_weight)\n",
    "        self.pretrained_time  = init_params((args.n_timeslot, args.sem_dim), pretrained_time)\n",
    "        \n",
    "        # --Placeholders\n",
    "        self.center_loc   = tf.placeholder(tf.int32, shape=[None], name='center_loc')\n",
    "        self.label_loc    = tf.placeholder(tf.int32, shape=[None, 1], name='label_loc') #specific shape for sampled_softamx_loss\n",
    "        self.weight_decay = tf.placeholder(tf.float32, shape=[None], name='weight_decay')\n",
    "        self.coor_center  = tf.placeholder(tf.float32, shape=[None,2], name='coor_center')\n",
    "        self.coor_label   = tf.placeholder(tf.float32, shape=[None,2], name='coor_label')\n",
    "        self.label_t      = tf.placeholder(tf.int32, shape=[None], name='label_t')\n",
    "        \n",
    "        # --Weights\n",
    "        slice_indices       = [args.sem_dim, args.geo_dim, args.free_dim]\n",
    "        self.softmax_biases = tf.get_variable('bias', initializer=tf.zeros([args.vocabulary_size]), trainable=False)\n",
    "        self.softmax_weights= tf.get_variable('weights', initializer=self.pretrained_weight)\n",
    "        self.embeddings     = tf.get_variable('embeddings', initializer=self.pretrained_emb)\n",
    "        self.time_embeddings= tf.get_variable('time_embeddings', initializer=self.pretrained_time)\n",
    "        self.sem_emb, self.geo_emb, self.free_emb = slice_func(self.embeddings, slice_indices)\n",
    "        self.sem_wht, self.geo_wht, self.free_wht = slice_func(self.softmax_weights, slice_indices)\n",
    "        \n",
    "        # --Retrive Embeddings\n",
    "        self.main_emb, self.context_emb = choose_emb(args, emb=self.embeddings, weight=self.softmax_weights)\n",
    "        self.emb_from_sem   = tf.nn.embedding_lookup(self.sem_emb, self.center_loc)\n",
    "        self.emb_from_geo_x = tf.nn.embedding_lookup(self.geo_emb, self.center_loc)\n",
    "        self.emb_from_geo_y = tf.nn.embedding_lookup(self.geo_emb, tf.squeeze(self.label_loc)) # due to the special shape of label_loc\n",
    "        self.emb_from_whole = tf.nn.embedding_lookup(self.main_emb, self.center_loc)\n",
    "        \n",
    "        # --Skipgram loss\n",
    "        self.skipgram_loss = tf.nn.sampled_softmax_loss(\n",
    "            inputs=self.emb_from_whole, \n",
    "            weights=self.context_emb, \n",
    "            biases=self.softmax_biases, labels=self.label_loc, \n",
    "            num_sampled=args.num_negative_sample, num_classes=args.vocabulary_size)\n",
    "        self.weighted_skipgram_loss = tf.reduce_mean(self.skipgram_loss*self.weight_decay)\n",
    "\n",
    "        # --GEO regularizer\n",
    "        self.euclidean_dis = tf.norm(self.coor_center-self.coor_label,ord='euclidean', axis=-1)\n",
    "        self.euclidean_sim = tf.exp(-1*self.euclidean_dis*args.geo_temp) #rescale it to (0,1)\n",
    "        cosine_sim = tf.reduce_mean(\n",
    "            tf.multiply(tf.nn.l2_normalize(self.emb_from_geo_x, axis=-1),\n",
    "                        tf.nn.l2_normalize(self.emb_from_geo_y, axis=-1)), axis=-1)\n",
    "        self.cosine_sim = 0.5*(cosine_sim+1) #rescale it to (0,1)\n",
    "        self.geo_loss_l2 = args.regulation_weight*tf.losses.mean_squared_error(labels=self.euclidean_sim, \n",
    "                                        predictions=self.cosine_sim)\n",
    "        self.geo_loss_xn = args.regulation_weight*crossentropy(y=self.euclidean_sim, \n",
    "                                        p=self.cosine_sim)\n",
    "        self.geo_loss = choose_geo_loss(args.geo_reg_type, loss_l2=self.geo_loss_l2, loss_xn=self.geo_loss_xn)\n",
    "        \n",
    "        # --Temporal regularizer\n",
    "        emb_dot_time = tf.matmul(self.emb_from_sem, tf.transpose(self.time_embeddings))\n",
    "        self.time_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=self.label_t, logits=emb_dot_time))\n",
    "        \n",
    "        # --Normalization\n",
    "        self.normalize_geo_emb_op = tf.assign(self.geo_emb, tf.nn.l2_normalize(self.geo_emb, axis=1))\n",
    "        self.normalize_sem_emb_op = tf.assign(self.sem_emb, tf.nn.l2_normalize(self.sem_emb, axis=1))\n",
    "        self.normalize_geo_wht_op = tf.assign(self.geo_wht, tf.nn.l2_normalize(self.geo_wht, axis=1))\n",
    "        self.normalize_sem_wht_op = tf.assign(self.sem_wht, tf.nn.l2_normalize(self.sem_wht, axis=1))\n",
    "        \n",
    "        # --Optimization\n",
    "        global_step_g = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(args.lr, global_step_g, 5000, 0.5, staircase=True) \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self.train_t = self.optimizer.minimize(self.time_loss)\n",
    "        self.train_skipgram = self.optimizer.minimize(self.skipgram_loss)\n",
    "        self.train_geo = self.optimizer.minimize(self.geo_loss)\n",
    "\n",
    "        # --Summaries\n",
    "        self.trainable_params = tf.trainable_variables()\n",
    "        self.all_params = tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(losses, sk, geo, t):\n",
    "    assert type(losses) is dict, 'losses is expected to be dict'\n",
    "    losses['geo'].append(geo)\n",
    "    losses['skipgram'].append(sk)\n",
    "    losses['time'].append(t)\n",
    "    return losses\n",
    "\n",
    "def compute_weight_decay(t1, t2, temp):\n",
    "    return np.exp(-1*((t1-t2)/60*temp)**2)\n",
    "\n",
    "def evaluate(emb, evaluator):\n",
    "    result = evaluator.evaluate(emb)\n",
    "    evaluator.update_history(res_dict=result)\n",
    "    evaluator.save_history()\n",
    "    return result\n",
    "\n",
    "def train(graph, sess, model, args, evaluator_emb, evaluator_weight, logger, dataloader, dataloader_time):\n",
    "    save_args(args)\n",
    "    losses = {'geo':[], 'skipgram':[], 'time':[]}\n",
    "    n_batch = 0\n",
    "    n_epoch = 0\n",
    "    tick0 = time.time()\n",
    "    \n",
    "    best_criteria = BestCriteria(['{}_f1_{}'.format(mode, k) for mode in ['sub', 'root'] for k in [1,5,10]])\n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver(model.all_params)\n",
    "        if args.resume:\n",
    "            sess = load_model_tf(saver, args, sess)\n",
    "            evaluator.load_history(args)\n",
    "        else:\n",
    "            logger.renew_log_file()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        logger.log('\\nStart training')\n",
    "        \n",
    "        while dataloader.get_epoch() < args.num_epoch:\n",
    "            if args.normalize_weight:\n",
    "                _ = sess.run([model.normalize_geo_emb_op, model.normalize_sem_emb_op,\n",
    "                              model.normalize_geo_wht_op, model.normalize_sem_wht_op])\n",
    "\n",
    "            epoch_tick = time.time()\n",
    "            emb, weight = sess.run([model.sem_emb, model.sem_wht])\n",
    "            result_emb = evaluate(emb, evaluator_emb)\n",
    "            result_weight = evaluate(weight, evaluator_weight)\n",
    "            result = result_emb if args.main_emb == 'emb' else result_weight\n",
    "            save_model_tf(saver, sess, args)\n",
    "            if best_criteria.should_save(result):\n",
    "                tmp = dict(result)\n",
    "                tmp['epoch'] = n_epoch\n",
    "                tmp['batch'] = n_batch\n",
    "                save_best_tf(saver, sess, args, {'args':vars(args), 'result':tmp})\n",
    "            #-- Optimization steps \n",
    "            while n_epoch >= dataloader.get_epoch():\n",
    "                center, context = next(dataloader.dg)\n",
    "                sk_loss, _, geo_loss, _ = sess.run([model.weighted_skipgram_loss, model.train_skipgram, model.geo_loss, model.train_geo],\n",
    "                          {model.center_loc:center.ids, \n",
    "                           model.label_loc:context.ids.reshape(-1,1),\n",
    "                           model.weight_decay: compute_weight_decay(center.timestmp, context.timestmp, args.time_temp),\n",
    "                           model.coor_center:center.coors, \n",
    "                           model.coor_label:context.coors})\n",
    "                \n",
    "                loc, time_label = next(dataloader_time.dg)\n",
    "                t_loss, _ = sess.run([model.time_loss, model.train_t],\n",
    "                         {model.center_loc:loc, model.label_t:time_label})\n",
    "                \n",
    "                losses = update(losses, sk=sk_loss, geo=geo_loss, t=t_loss)\n",
    "                \n",
    "                if n_batch % 100 == 0:\n",
    "                    losses = {k:np.mean(v) for k, v in losses.items()}\n",
    "                    evaluator_emb.update_history(losses=losses)\n",
    "                    evaluator_weight.update_history(losses=losses)\n",
    "                    logstr = '[{}] LOSS '.format(n_batch) + \"\".join(['{} : {:.6f} '.format(k, v) for k, v in losses.items()])\n",
    "                    losses = {'geo':[], 'skipgram':[], 'time':[]}\n",
    "                    logger.log(logstr)\n",
    "                    \n",
    "                n_batch += 1\n",
    "            #-----------------------\n",
    "            n_epoch += 1\n",
    "            logstr = '#'*50+'\\n'\n",
    "            logstr += 'Ecpoh {}, used time: {}, eval: {}'.format(n_epoch, time.time()-epoch_tick, result)\n",
    "            logger.log(logstr)\n",
    "    logger.log('FINISH, USED TIME:{}'.format(time.time()-tick0))\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.time_temp = 0.01\n",
    "# args.geo_temp = 10\n",
    "# args.main_emb = 'emb'\n",
    "# args.regulation_weight = 10\n",
    "# args.num_epoch = 30\n",
    "# args.resume = False\n",
    "# args.n_processes = 3\n",
    "# args.batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved args to log_test/args.json\n",
      "\n",
      "Start training\n",
      "eval distance\n",
      "Job Done, used time 2.911085367202759\n",
      "Job Done, used time 1.6089904308319092\n",
      "eval translation\n",
      "saved history to log_test/emb_history.pk\n",
      "eval distance\n",
      "Job Done, used time 2.7766852378845215\n",
      "Job Done, used time 1.6020898818969727\n",
      "eval translation\n",
      "saved history to log_test/weight_history.pk\n",
      "Saved model to log_test/saved/model.ckpt\n",
      "Saved BEST model to log_test/best/model.ckpt\n",
      "[0] LOSS geo : 1.463893 time : 1.925902 skipgram : 2.332961 \n",
      "[100] LOSS geo : 1.407218 time : 26.781513 skipgram : 7.538828 \n",
      "[200] LOSS geo : 1.415469 time : 25.976307 skipgram : 8.395305 \n",
      "[300] LOSS geo : 1.404142 time : 25.259361 skipgram : 7.223943 \n",
      "[400] LOSS geo : 1.408293 time : 22.245857 skipgram : 6.958654 \n",
      "[500] LOSS geo : 1.399266 time : 19.082109 skipgram : 7.394409 \n",
      "[600] LOSS geo : 1.392944 time : 16.423002 skipgram : 6.208087 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-dc900ff586c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTSkipgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-264-3bc29cc32f82>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(graph, sess, model, args, evaluator_emb, evaluator_weight, logger, dataloader, dataloader_time)\u001b[0m\n\u001b[1;32m     57\u001b[0m                            \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompute_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                            \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoor_center\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                            model.coor_label:context.coors})\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/py3_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/py3_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/py3_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/py3_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/py3_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/py3_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(train_data, args)\n",
    "dataloader_time = DataLoader_time(data, args, idx)\n",
    "evaluator_emb = Evaluator(args, dicts, mode='emb')\n",
    "evaluator_weight = Evaluator(args, dicts, mode='weight')\n",
    "logger = Logger(os.path.join(args.LOG_DIR, 'log_txt'))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    model = STSkipgram(args)\n",
    "    sess = tf.Session(graph=graph, config=config)\n",
    "state = train(graph, sess, model, args, evaluator_emb, evaluator_weight, logger, dataloader, dataloader_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with graph.as_default():\n",
    "#     sess = tf.Session(graph=graph, config=config)\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     center, context = next(dataloader.dg)\n",
    "#     sk_loss, _, geo_loss, _ = sess.run([model.weighted_skipgram_loss, model.train_skipgram, model.geo_loss, model.train_geo],\n",
    "#                           {model.center_loc:center.ids, \n",
    "#                            model.label_loc:context.ids.reshape(-1,1),\n",
    "#                            model.weight_decay: compute_weight_decay(center.timestmp, context.timestmp, args.time_temp),\n",
    "#                            model.coor_center:center.coors, \n",
    "#                            model.coor_label:context.coors})\n",
    "\n",
    "#     loc, time_label = next(dataloader_time.dg)\n",
    "#     t_loss, _ = sess.run([model.time_loss, model.train_t],\n",
    "#                          {model.center_loc:loc, model.label_t:time_label})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_env",
   "language": "python",
   "name": "py3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
